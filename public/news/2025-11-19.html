
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-19</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-19</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12404" style="color:#4ea8ff;">SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SynthGuard presents an open-source detection platform leveraging multimodal LLMs combined with traditional forensic detectors to identify AI-generated images, audio, and video with explainable reasoningâ€”addressing critical gaps in existing closed-source tools that lack transparency and multimodal coverage. The platform's key technical innovation is integrating LLM-based analysis with classical detection methods to provide interpretable outputs that help users understand detection decisions, directly countering the growing threat of indistinguishable synthetic media for misinformation and identity fraud. This represents a shift toward democratized, interpretable deepfake detection infrastructure rather than proprietary black-box solutions, enabling researchers and developers to audit detection logic and build on unified multimodal foundations.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14582" style="color:#4ea8ff;">OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>OmniZip introduces a training-free framework for compressing audio-video token sequences in omnimodal LLMs by using audio salience to guide dynamic video token pruning, leveraging cross-modal similarity and spatio-temporal compression schemes. The approach addresses a critical computational bottleneck in unified audio-video understanding by dynamically computing audio retention scores that inform which video tokens to preserve, enabling faster inference without requiring model retraining. This represents a practical solution for deploying resource-intensive multimodal models where joint audio-visual processing is essential.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and" style="color:#4ea8ff;">Google unveils Gemini 3 claiming the lead in math, science, multimodal and agentic AI benchmarks </a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Google's Gemini 3 represents a comprehensive frontier model portfolio emphasizing agentic capabilities, featuring specialized variants (Pro, Deep Think) alongside integrated agent frameworks and claiming leadership across math, science, and multimodal benchmarks. The release prioritizes developer accessibility through multiple deployment channels (Vertex AI, APIs, IDE integrations) while introducing Gemini Agent for autonomous multi-step task execution and Google Antigravity as an agent-centric development environment. This positions Gemini 3 as a shift toward production-ready agentic systems rather than purely generative models, with closed-source distribution maximizing Google's platform control.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Yanyun-3 demonstrates the first cross-platform autonomous agent for strategy games by combining Qwen2.5-VL's multimodal reasoning with UI-TARS for precise UI interaction, successfully handling complex tasks like target localization and resource allocation across heterogeneous game environments. The framework's ablation studies systematically evaluate different input modalities (static images, sequences, video), establishing which data representations best support robust generalization across diverse interfaces and dynamic battlefield conditions. This work advances VLM application from static reasoning into real-time, high-stakes human-computer interaction scenarios with significant implications for developing domain-agnostic autonomous agents.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent introduces a novel episodic memory architecture that structures video events with explicit causal and temporal relationships to overcome MLLMs' token limitations and long-term dependency challenges in video understanding. The framework operates via a Perception-Action-Reflection cycle with a Memory Manager that dynamically retrieves relevant context, enabling more efficient and coherent reasoning across extended video sequences. This approach represents a meaningful advance in addressing scalability constraints inherent to transformer-based video models by shifting from raw token processing to structured semantic memory.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12072" style="color:#4ea8ff;">ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#10b981;">âš¡ 92</span></div>
  <p>ProAV-DiT addresses synchronized audio-video generation by converting audio into video-like spectral representations and projecting both modalities into a unified latent space via orthogonal decomposition, enabling fine-grained spatiotemporal alignment. The approach uses a Multi-scale Dual-stream Spatio-Temporal Autoencoder with multi-scale attention mechanisms to enhance temporal coherence and cross-modal fusion, significantly reducing computational overhead compared to existing diffusion-based video generation methods. This tackles a critical challenge in sounding video synthesisâ€”structural misalignment between modalitiesâ€”making it potentially valuable for efficient synchronized media generation in production pipelines.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>Uni-MoE-2.0-Omni introduces a dynamic-capacity MoE architecture with shared/routed/null experts and Omni-Modality 3D RoPE for efficient omnimodal processing across 10 input types (image, text, speech, video), built atop Qwen2.5-7B with progressive training and iterative reinforcement. The model demonstrates a language-centric approach to unified multimodal understanding, reasoning, and generation while maintaining computational efficiency through expert routing strategies. This open-source contribution represents a significant step toward practical omnimodal systems that can both consume and generate across modalities without massive scaling penalties.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13530" style="color:#4ea8ff;">Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>This paper introduces a protocol for collecting a synchronized multimodal dataset (audio, video, physiological signals) from 70+ participants with varying social anxiety levels during Wizard-of-Oz human-robot interactions, addressing the scarcity of affect-annotated datasets needed for anxiety detection in HRI contexts. The complementary nature of multimodal signals enables more robust affective state classification than single-modality approaches, with direct applications to developing emotionally-aware social robots for therapeutic or assistive interventions. This work bridges AI/robotics with clinical psychology, providing infrastructure for training affect-adaptive interaction models that can recognize and respond to anxiety-related behavioral markers in real-time.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM introduces an end-to-end framework combining a novel Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal foundation models for direct acoustic-to-intent recognition in long dialogues, addressing speaker utterance interdependencies without requiring intermediate representations. The system employs an adaptive semi-supervised learning strategy with confidence-aware pseudo-labeling (dual-threshold filtering on global/class confidence + entropy-based sample selection) to tackle annotation scarcity, enabling effective learning from unlabeled audio data. This approach demonstrates practical applicability across proprietary and benchmark datasets, potentially advancing real-world applications like customer call analysis where labeled dialogue data is limited.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14143" style="color:#4ea8ff;">SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SMART introduces a novel MLLM-based video moment retrieval framework that enhances temporal localization by integrating audio modality and implementing shot-aware token compression to maintain fine-grained temporal granularity while reducing redundancy. The approach addresses limitations of existing single-modality methods by leveraging multimodal (audio-visual) representations structured at the shot level, enabling more precise localization in complex, untrimmed videos. This technique has immediate implications for video understanding tasks requiring accurate temporal grounding, particularly in scenarios where audio context is semantically important for query disambiguation.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.01711" style="color:#4ea8ff;">GAIS: Frame-Level Gated Audio-Visual Integration with Semantic Variance-Scaled Perturbation for Text-Video Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>GAIS addresses text-to-video retrieval by introducing two complementary mechanisms: Frame-level Gated Fusion (FGF) for adaptive audio-visual integration under textual guidance, and Semantic Variance-Scaled Perturbation (SVSP) for semantically-aware regularization of text embeddings. This dual approach tackles the core limitation of existing methodsâ€”underutilized audio semantics and coarse fusion strategiesâ€”by enabling fine-grained temporal selection and improved embedding stability. The framework is particularly relevant for practitioners building multimodal retrieval systems where audio-visual-text alignment is critical, offering a principled way to balance modality contributions rather than relying on generic fusion strategies.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2510.12089" style="color:#4ea8ff;">Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Playmate2 advances audio-driven animation by combining diffusion transformers with LoRA-based training and reward feedback to achieve improved lip-sync accuracy and temporal coherence for arbitrarily long videos. The key innovation is a training-free multi-character animation method using Mask Classifier-Free Guidance, enabling efficient generation without retraining while preserving foundation model capabilities through position shift inference during long sequence generation.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://www.marktechpost.com/2025/11/17/uni-moe-2-0-omni-an-open-qwen2-5-7b-based-omnimodal-moe-for-text-image-audio-and-video-understanding/" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: An Open Qwen2.5-7B Based Omnimodal MoE for Text, Image, Audio and Video Understanding</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    MarkTechPost
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Uni-MoE-2.0-Omni is an open-source omnimodal mixture-of-experts model built on Qwen2.5-7B that unifies text, image, audio, and video understanding through a language-centric reasoning architecture, enabling efficient multimodal processing without proprietary dependencies. The MoE approach allows selective expert activation across modalities, reducing computational overhead while maintaining performance across diverse input typesâ€”a significant advancement for practitioners seeking open alternatives to closed multimodal systems.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11614" style="color:#4ea8ff;">Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>FPGAs are emerging as a compelling alternative to GPUs for AI workloads that prioritize deterministic latency, energy efficiency, and hardware customizationâ€”enabling reconfigurable pipelines for convolutions and attention mechanisms with direct algorithm-to-logic mapping unavailable in fixed architectures. Their ability to run inference at the edge with sub-millisecond predictability, reduced bandwidth requirements, and integrated SoC capabilities addresses critical gaps in cloud-dependent GPU deployments, particularly for privacy-sensitive and real-time applications. This positions FPGAs as strategic accelerators for specialized AI inference rather than general-purpose compute, especially where hardware-software codesign and fine-grained control justify the increased development complexity.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11743" style="color:#4ea8ff;">Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p># Summary

This work introduces a curiosity-driven Mixture-of-Experts (MoE) framework that uses Bayesian epistemic uncertainty to intelligently route audio through heterogeneous quantized experts (ternary, 1-16 bit, and post-training quantized), achieving aggressive 4-bit quantization while maintaining 99.9% accuracy with 4x compression. The key innovation is using uncertainty-based routing to dramatically reduce MoE latency variance by 82% (230msâ†’29ms std dev), enabling predictable, energy-efficient inference on battery-constrained edge devices for audio classification tasks.</p>
</article>
</body>
</html>