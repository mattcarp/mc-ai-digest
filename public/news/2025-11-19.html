
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-19</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-19</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and" style="color:#4ea8ff;">Google unveils Gemini 3 claiming the lead in math, science, multimodal and agentic AI benchmarks </a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#6b7280;">âš¡ 28</span></div>
  <p>Google's Gemini 3 family advances frontier AI across multiple dimensions, with the flagship Pro model claiming leadership in mathematical reasoning, scientific benchmarks, and multimodal tasks, while introducing Gemini 3 Deep Think for enhanced chain-of-thought reasoning and Gemini Agent for autonomous multi-step task execution. The release emphasizes agentic capabilities through integration with Google Antigravity (an agent-first development environment) and multi-framework API access, positioning structured reasoning and tool-use orchestration as core competitive advantages over existing models. Developers gain access through familiar Google infrastructure (Vertex AI, CLI, IDEs), but the closed-source, proprietary-only approach constrains academic research and limits deployment flexibility compared to open-weight alternatives.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://www.marktechpost.com/2025/11/17/uni-moe-2-0-omni-an-open-qwen2-5-7b-based-omnimodal-moe-for-text-image-audio-and-video-understanding/" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: An Open Qwen2.5-7B Based Omnimodal MoE for Text, Image, Audio and Video Understanding</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    MarkTechPost
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Uni-MoE-2.0-Omni is an open-source omnimodal mixture-of-experts model built on Qwen2.5-7B that unifies text, image, audio, and video understanding while maintaining computational efficiency through MoE routing. The system extends language-centric multimodal reasoning by training a single architecture from scratch to handle diverse modalities, enabling practical omnimodal inference without requiring separate specialized models. This open approach addresses a key gap in accessible multimodal AI by combining efficiency with comprehensive modality support, making it relevant for developers building production systems requiring cross-modal reasoning capabilities.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM presents an end-to-end framework combining a novel Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal foundation models (Qwen2.5-Omni-7B) to perform direct acoustic-to-intent inference in long multi-speaker dialogues, addressing the challenge of complex inter-speaker dependencies. The system introduces an adaptive semi-supervised learning strategy with confidence-aware pseudo-label generation (dual-threshold filtering) and entropy-based sample selection to tackle data scarcity, enabling effective learning from unlabeled audio. This graph-informed approach bridges foundational models with dialogue structure modeling, offering practical improvements for real-world applications like call center analysis where annotated dialogue data remains limited.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11910" style="color:#4ea8ff;">Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**Query-Aware Token Selector (QTSplus)** addresses the computational bottleneck in long-video MLLMs by dynamically selecting task-relevant visual tokens rather than processing all frames uniformlyâ€”using cross-attention scoring and query-complexity-aware retention budgets to reduce token count while preserving essential information. This lightweight module operates between vision encoders and LLMs, achieving linear-to-sublinear scaling benefits for attention complexity, memory, and latency without requiring architectural retraining. The differentiable training mechanism and hard-gating inference strategy enable practical deployment while maintaining competitive accuracy on long-form video understanding tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent addresses long-video understanding in MLLMs by introducing Schematic and Narrative Episodic Memoryâ€”a structured representation that models events and their causal/temporal relations to overcome token limitations and capture long-term dependencies. The framework operates through a Perception-Action-Reflection cycle with a Memory Manager that retrieves relevant episodic context, enabling more robust reasoning about complex event relationships in extended videos. This approach fundamentally shifts from sequential token processing to structured semantic memory, potentially improving performance across long-form video understanding tasks while maintaining computational efficiency.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13655" style="color:#4ea8ff;">OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>OlmoEarth introduces a spatio-temporal foundation model specifically optimized for Earth observation data, combining domain-tailored self-supervised learning, masking strategies, and loss functions to handle the unique challenge of data that is simultaneously spatial, sequential, and multimodal. The model achieves SOTA results across 24 benchmarks and real-world tasks, outperforming 12 competing foundation models with particular strength in embedding tasks (15/24) and fine-tuning scenarios (19/29). Critically, the accompanying platform democratizes frontier EO models for NGOs and non-profits, shifting Earth observation from niche applications to accessible tools for climate/humanitarian impact.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#10b981;">âš¡ 85</span></div>
  <p>Uni-MoE-2.0-Omni presents a language-centric omnimodal model built on Qwen2.5-7B using dynamic-capacity MoE with shared/routed/null experts and Omni-Modality 3D RoPE for cross-modal alignment, enabling unified understanding and generation across text, image, and speech modalities. The model employs progressive training with iterative reinforcement and curated multimodal data matching to optimize performance across 10 cross-modal input types while maintaining computational efficiency. This architecture represents a practical approach to scaling omnimodal capabilities through MoE-based expert routing rather than scaling dense parameters, with potential implications for efficient multi-task foundation models.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Yanyun-3 is a novel VLM-based agent framework that combines Qwen2.5-VL's multimodal reasoning with UI-TARS' precise action execution to autonomously operate across heterogeneous strategy game platformsâ€”the first demonstrated cross-platform generalization at this scale. The system addresses core gameplay challenges including target localization, resource allocation, and area control, with systematic ablations revealing how different multimodal inputs (static images, sequences, videos) impact performance in dynamic, complex HCI scenarios. This work validates VLMs as viable foundations for autonomous agents in unstructured, real-time environments with high visual complexity and dense decision-making requirements.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.04953" style="color:#4ea8ff;">APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>APVR introduces a training-free, hierarchical retrieval framework for hour-level video understanding in MLLMs that overcomes memory constraints through dual-stage filtering: Pivot Frame Retrieval uses query expansion and spatio-semantic scoring to identify relevant frames, while Pivot Token Retrieval performs query-aware token selection within those frames. This approach maintains performance comparable to full-video processing while dramatically reducing computational overhead, enabling practical long-video analysis without model retraining or infrastructure scaling.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>READ addresses the critical inference speed bottleneck in diffusion-based talking head generation through a three-pronged technical approach: temporal VAE compression of video latents, a novel Speech Autoencoder for temporally-aligned audio encoding, and an optimized Audio-to-Video Diffusion Transformer backbone. This architecture enables real-time synthesis while maintaining audio-visual synchronizationâ€”a significant practical advance for deployment in applications requiring low-latency talking head generation. The work exemplifies how architectural innovations in latent space compression and cross-modal alignment can make diffusion models feasible for real-time video synthesis tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    eess.AS updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>READ addresses the critical inference speed bottleneck of diffusion-based talking head generation through a three-pronged approach: temporal VAE compression of video latents, a Speech Autoencoder for temporally-aligned audio encoding, and an optimized Audio-to-Video Diffusion Transformer backbone. This architecture enables real-time synthesis by dramatically reducing token counts while maintaining audio-visual synchronization in compressed latent space. The work represents a practical advancement toward deployable diffusion models for interactive talking head applications, shifting from theoretical superiority to production viability.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/musks-xai-launches-grok-4-1-with-lower-hallucination-rate-on-the-web-and" style="color:#4ea8ff;">Musk's xAI launches Grok 4.1 with lower hallucination rate on the web and apps â€” no API access (for now)</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#6b7280;">âš¡ 18</span></div>
  <p>xAI's Grok 4.1 demonstrates significant architectural improvements including reduced hallucination rates, faster reasoning, and enhanced emotional intelligence, now accessible via web and mobile but without API availability. The model currently leads public benchmarks against Anthropic and OpenAI offerings (though pre-Gemini 3), with xAI publishing evaluation methodology transparency through a technical white paperâ€”positioning it as a competitive contender before Google's Gemini 3 launch.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.10979" style="color:#4ea8ff;">PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>This work identifies a fundamental instability in video LLMs stemming from how Rotary Position Embeddings (RoPE) extend to temporal dimensionsâ€”specifically, multimodal RoPE creates frame-scale ripples in the time kernel that cause erratic attention shifts. The authors propose Phase Aggregated Smoothing (PAS), a training-free post-processing technique that applies phase offsets across attention heads and aggregates outputs to smooth the temporal kernel, effectively decoupling attention from timing perturbations while preserving positional encoding structure. The approach is elegant in its simplicity and has immediate practical value for practitioners deploying video LLMs, as it requires no model retraining or architectural changes.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11006" style="color:#4ea8ff;">MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**Summary:**

MSMT-FN introduces a multi-segment multi-task fusion architecture specifically optimized for classifying customer sentiment and purchasing intent from marketing call audio, addressing the scalability challenge of processing large audio datasets. The approach demonstrates consistent improvements over SOTA methods on both the authors' proprietary MarketCalls dataset and established multimodal benchmarks (CMU-MOSI/MOSEI, MELD), with code and dataset availability for reproducibility.

**Key technical implication:** The multi-task fusion design suggests leveraging complementary learning objectives (sentiment + intent classification) to improve feature representations, a pattern increasingly effective for audio understanding tasks with limited labeled data.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11124" style="color:#4ea8ff;">AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**AV-Dialog** introduces the first audio-visual dialogue framework that leverages both acoustic and visual cues for robust speaker tracking and turn-taking prediction in noisy multi-speaker environments, using acoustic tokenization combined with multi-task, multi-stage training on diverse datasets. The system significantly outperforms audio-only baselines in transcription accuracy, turn-boundary detection, and dialogue coherence, demonstrating that visual information (likely lip-sync and speaker identification) is critical for naturally-grounded conversational flow in real-world scenarios. This represents a meaningful step toward embodied dialogue agents, with practical implications for voice assistants operating in challenging acoustic conditions.</p>
</article>
</body>
</html>