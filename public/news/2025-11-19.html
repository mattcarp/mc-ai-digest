
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-19</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-19</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12404" style="color:#4ea8ff;">SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SynthGuard presents an open-source, multimodal detection platform combining traditional deepfake detectors with MLLMs to identify AI-generated images and audio while providing explainable inferenceâ€”addressing critical gaps in transparency and educational accessibility that plague existing closed-source tools. The platform's unified approach across multiple modalities and emphasis on interpretability aims to combat synthetic media's erosion of trust while enabling users to understand detection reasoning, which is essential as generation quality approaches photorealism. This represents a meaningful shift toward democratizing detection capabilities beyond proprietary solutions, though real-world efficacy against state-of-the-art generators and the computational overhead of MLLM-based approaches remain key technical considerations.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14582" style="color:#4ea8ff;">OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>OmniZip introduces a training-free, audio-guided dynamic token compression framework that accelerates omnimodal LLM inference by leveraging audio salience to guide video token pruning, using cross-modal similarity metrics and interleaved spatio-temporal compression to reduce computational overhead without retraining. The key innovation is treating audio as an information density anchor for multimodal compression, enabling joint optimization of audio-video token sequencesâ€”a gap existing methods fail to address in unified audio-visual understanding tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and" style="color:#4ea8ff;">Google unveils Gemini 3 claiming the lead in math, science, multimodal and agentic AI benchmarks </a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Google released Gemini 3, a comprehensive model family claiming state-of-the-art performance across math, science, multimodal, and agentic AI benchmarks, with specialized variants including Gemini 3 Pro, Deep Think (enhanced reasoning), and integrated agent capabilities for multi-step task execution. The models are proprietary and distributed exclusively through Google's ecosystem (Vertex AI, AI Studio, APIs) alongside a new agent-first development environment called Google Antigravity. This positions Gemini 3 as Google's most significant AI release since 2023, directly addressing competitive pressure in reasoning, scientific problem-solving, and autonomous task planning capabilities.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Yanyun-3 demonstrates the first successful cross-platform strategy game agent by combining Qwen2.5-VL for vision-language reasoning with UI-TARS for precise UI interaction, enabling autonomous execution of complex tasks like target localization and resource allocation across heterogeneous game environments. The work systematically evaluates multimodal input strategies (static images, sequences, and video) to optimize VLM performance in dynamic, real-time decision-making scenariosâ€”addressing a significant gap in applying foundation models to complex human-computer interaction beyond traditional benchmarks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent addresses long-video understanding in MLLMs through an episodic memory architecture that structures events and their causal/temporal relationships into compressed context, circumventing token limitations inherent in transformer-based models. The framework employs a Perception-Action-Reflection cycle with a Memory Manager that retrieves relevant episodic context, enabling robust reasoning over complex temporal dependencies that existing frame-sampling or hierarchical approaches fail to capture. This represents a meaningful shift from treating videos as flat sequences toward dynamic memory-augmented reasoning, with implications for autonomous video analysis, embodied AI systems, and reduced computational overhead in long-context understanding.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12072" style="color:#4ea8ff;">ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#10b981;">âš¡ 88</span></div>
  <p>ProAV-DiT addresses synchronized audio-video generation by converting audio into video-like representations and projecting both modalities into a unified latent space via orthogonal decomposition, enabling structural alignment between the inherently misaligned modalities. The approach combines a Multi-scale Dual-stream Spatio-Temporal Autoencoder with multi-scale attention mechanisms for fine-grained spatiotemporal modeling, significantly reducing computational overhead compared to processing raw multimodal data. This represents a meaningful advance for efficient sounding video generation, with practical applications in content creation and multimodal synthesis where temporal coherence between audio and video is critical.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>Uni-MoE-2.0-Omni introduces a language-centric omnimodal model built on Qwen2.5-7B with a novel dynamic-capacity MoE architecture featuring shared/routed/null experts optimized for 10 cross-modal inputs and Omni-Modality 3D RoPE for spatio-temporal alignment. The model combines progressive training with iterative reinforcement and curated multimodal data matching to enable unified understanding and generation across text, images, and speech while maintaining computational efficiency.

**Key implication for developers**: This represents a practical scaling approach for true omnimodal systems, demonstrating how MoE routing can handle heterogeneous modalities more efficiently than dense architecturesâ€”relevant for building resource-constrained production systems handling diverse input types.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13530" style="color:#4ea8ff;">Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>This paper outlines a protocol for creating a multimodal dataset capturing social anxiety manifestations during human-robot interaction, integrating synchronized audio, video, and physiological signals from 70+ participants stratified by anxiety levels. The work addresses a critical gap in affect-detection research by providing complementary signal modalities that enable more robust anxiety classification in HRI contexts, with potential applications in anxiety assessment and adaptive robotic interventions. The Wizard-of-Oz experimental design allows controlled interaction scenarios while maintaining ecological validity for training next-generation affect-aware social robots.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**DialogGraph-LLM** combines a novel Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal foundation models to perform end-to-end acoustic-to-intent inference directly from audio, addressing the challenge of recognizing speaker intent in long dialogues with complex utterance dependencies. The framework introduces an adaptive semi-supervised learning strategy using dual-threshold confidence filtering and entropy-based sample selection to generate reliable pseudo-labels from unlabeled data, mitigating the scarce annotation problem. This approach enables practical intent recognition at scale while leveraging both the relational structure of dialogue and the semantic understanding of large foundation models.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14143" style="color:#4ea8ff;">SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SMART introduces a shot-aware MLLM framework for video moment retrieval that integrates audio cues and implements Shot-aware Token Compression to preserve fine-grained temporal semantics while reducing computational redundancy. By combining multimodal audio-visual features with shot-level temporal structure, the approach addresses limitations of existing methods that rely on coarse single-modality analysis of untrimmed videos. This has practical implications for efficient processing of longer, complex videos where shot boundaries provide natural anchors for more precise temporal localization.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.01711" style="color:#4ea8ff;">GAIS: Frame-Level Gated Audio-Visual Integration with Semantic Variance-Scaled Perturbation for Text-Video Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>GAIS introduces a dual-pronged approach to text-video retrieval by combining Frame-level Gated Fusion (FGF) for adaptive audio-visual integration under textual guidance with Semantic Variance-Scaled Perturbation (SVSP) for semantics-aware regularization of text embeddings. This framework addresses a key limitation in existing methodsâ€”the underutilization of audio semantics and coarse fusion strategiesâ€”by enabling fine-grained temporal selection while improving embedding stability and cross-modal discrimination. The complementary design of selective fusion and semantic-aware perturbation represents a meaningful advance in multimodal alignment for retrieval tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2510.12089" style="color:#4ea8ff;">Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Playmate2 addresses multi-character audio-driven animation using a DiT-based framework with LoRA training and position shift inference for arbitrary-length video generation, while introducing a novel training-free Mask Classifier-Free Guidance approach that combines reward feedback to improve lip-sync accuracy and temporal coherence. The method achieves multi-character animation without additional training by leveraging partial parameter updates and classifier-free guidance, enabling practical deployment of complex talking head scenarios. This represents a significant advancement in controllable video generation by eliminating the need for character-specific fine-tuning while maintaining quality across extended sequences.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11614" style="color:#4ea8ff;">Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p># FPGAs as an AI Acceleration Alternative to GPUs

FPGAs offer reconfigurable hardware that enables deterministic, low-latency AI inference with fine-grained optimization for specific workloads, outperforming fixed GPU architectures in energy efficiency and customization when mapping algorithms like convolutions and attention mechanisms directly to device logic. Their ability to integrate as SoCs with embedded processors enables edge inference near sensors, reducing bandwidth requirements and latency compared to cloud-based GPU processing. This positions FPGAs as a strategic complement to GPUs for applications prioritizing predictable performance, power efficiency, and privacy-critical deployment scenarios.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11743" style="color:#4ea8ff;">Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>This paper proposes a Bayesian uncertainty-guided Mixture-of-Experts (MoE) framework that routes inputs across heterogeneous quantized experts (ternary BitNet, 1-16 bit BitLinear) to achieve aggressive 4-bit quantization while maintaining 99.9% accuracy parity with 16-bit models on audio tasks. The key innovation is curiosity-driven routing based on epistemic uncertainty, which dramatically reduces inference latency variance by 82% (230msâ†’29ms std dev), enabling predictable real-time inference on resource-constrained edge devices with 4x compression and 41% energy savings. This addresses a critical deployment bottleneck: prior MoE approaches suffer from high latency unpredictability under quantization, making them unsuitable for latency-sensitive embedded applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11751" style="color:#4ea8ff;">Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>Concept-RuleNet combines multi-agent neurosymbolic reasoning with vision-language models by grounding symbolic rules directly in visual concepts extracted from training data, rather than relying solely on task labelsâ€”addressing both VLM hallucination issues and the weak grounding problem in existing neurosymbolic approaches. The system uses a multimodal concept generator to discover discriminative visual patterns first, then conditions symbol discovery on these grounded concepts to create interpretable, executable reasoning rules that mitigate label bias and improve out-of-distribution robustness. This approach bridges the gap between neural perception accuracy and symbolic reasoning transparency, offering developers a framework for building more reliable and explainable vision-language systems.</p>
</article>
</body>
</html>