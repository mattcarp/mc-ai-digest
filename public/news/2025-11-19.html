
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-19</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-19</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12404" style="color:#4ea8ff;">SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SynthGuard is an open-source platform that leverages multimodal LLMs alongside traditional detectors to identify AI-generated images and audio, prioritizing explainability and transparency over proprietary black-box solutions. The system addresses critical gaps in existing deepfake detection tools by providing unified cross-modal support and educational insights into detection reasoning, making synthetic media forensics more accessible and interpretable for end-users. This approach represents a shift toward democratized AI detection infrastructure with interpretable decision-makingâ€”crucial for maintaining media authenticity verification as generation quality approaches imperceptibility.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14582" style="color:#4ea8ff;">OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>OmniZip introduces a training-free framework for joint audio-video token compression in omnimodal LLMs by leveraging audio salience to guide dynamic video token pruning through cross-modal similarity scoring and spatio-temporal compression. The approach addresses a critical computational bottleneck in unified audio-video understanding by preserving audio-anchored multimodal information while achieving inference acceleration without retraining. This represents a practical solution for efficient omnimodal processing that could enable real-time audio-video understanding applications at scale.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and" style="color:#4ea8ff;">Google unveils Gemini 3 claiming the lead in math, science, multimodal and agentic AI benchmarks </a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Google's Gemini 3 represents a comprehensive multimodal frontier model family with specialized variants including a reasoning-enhanced "Deep Think" mode and native agentic capabilities, claiming benchmark leadership across math, science, and multimodal tasks. The release emphasizes agent-first architecture with integrated tools like Gemini Agent for orchestrating multi-step workflows and Antigravity (Google's new development environment), suggesting a strategic shift toward autonomous task execution rather than pure language modeling. Availability is restricted to Google's proprietary ecosystem (Vertex AI, AI Studio, APIs), limiting independent evaluation and raising questions about reproducibility claims relative to open competitors like Claude 3.5 or open-source alternatives.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Yanyun-3 introduces a unified VLM-based agent framework that successfully operates across heterogeneous strategy game platforms by combining Qwen2.5-VL's multimodal reasoning with UI-TARS for precise UI interaction, tackling the under-explored challenge of applying VLMs to complex real-time HCI scenarios. The framework demonstrates robust generalization across diverse interfaces and dynamic conditions through systematic evaluation of multimodal input modalities (static images, sequences, video), establishing a benchmark for autonomous cross-platform game operation that could generalize to other complex GUI automation tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>GCAgent addresses long-video understanding in MLLMs by introducing Schematic and Narrative Episodic Memory that structurally encodes events and their causal/temporal relations into compressed context, circumventing token limitations while preserving long-term dependencies. The framework operates via a Perception-Action-Reflection cycle with a Memory Manager for context-aware retrieval, enabling robust reasoning over complex event relationships that traditional methods fail to capture. This approach represents a practical shift from token-limited frame sampling to semantic event graph abstraction, offering potential applications in extended video analysis where global context is critical.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12072" style="color:#4ea8ff;">ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>ProAV-DiT addresses synchronized audio-video generation by converting audio into video-like representations and projecting both modalities into a unified latent space via orthogonal decomposition in a Multi-scale Dual-stream Spatio-Temporal Autoencoder. The approach employs multi-scale temporal self-attention and cross-modal fusion mechanisms within a diffusion transformer framework to maintain temporal coherence while reducing computational overhead. This tackles a key challenge in sounding video generationâ€”structural misalignment between modalitiesâ€”making efficient end-to-end audiovisual synthesis more tractable for practical applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>Uni-MoE-2.0-Omni introduces a dynamic-capacity MoE architecture with shared/routed/null experts and Omni-Modality 3D RoPE for efficient cross-modal alignment across 10 input types, built on Qwen2.5-7B and capable of generating images, text, and speech. The model leverages progressive training with iterative reinforcement and curated multimodal data matching to balance computational efficiency with omnimodal reasoning capabilities in a fully open-source framework. This represents a significant advance in language-centric multimodal systems by addressing the scaling challenge of handling diverse modalities through expert routing rather than dense model expansion.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13530" style="color:#4ea8ff;">Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>This paper introduces a standardized protocol for collecting multimodal datasets (audio, video, physiological signals) from 70+ participants during human-robot interactions, stratified by social anxiety levels, to enable affect-adaptive robotic systems. The synchronized multimodal approach addresses a critical gap in datasets needed for training robust anxiety detection models, leveraging complementary signal modalities to capture the complex behavioral manifestations of social anxiety. This work has significant implications for developing socially-aware robots capable of real-time emotional state recognition and adaptive interaction strategies, potentially advancing therapeutic and social support applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM introduces an end-to-end framework combining a Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal foundation models to directly infer speaker intent from audio while capturing complex inter-speaker dependencies. The system addresses data scarcity through an adaptive semi-supervised learning approach using confidence-aware pseudo-labeling with dual-threshold filtering and entropy-based sample selection to leverage unlabeled data effectively. This work bridges acoustic signal processing with LLM reasoning for dialogue understanding, enabling practical applications in domains like financial call analysis where annotated dialogue data is limited.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14143" style="color:#4ea8ff;">SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SMART introduces a novel MLLM-based video moment retrieval framework that addresses temporal precision limitations by integrating audio modality alongside video and implementing shot-level token compression to preserve fine-grained temporal structure while reducing computational redundancy. The key innovation lies in multi-shot aware processing that selectively retains high-information tokens per shot, enabling more accurate localization of video segments from natural language queries than existing vision-only or coarse-temporal methods.

**Practical implication**: This approach significantly advances multimodal video understanding for applications requiring precise temporal grounding (video editing, content summarization, retrieval systems) by leveraging underutilized audio cues within an efficient token-selective architecture.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.01711" style="color:#4ea8ff;">GAIS: Frame-Level Gated Audio-Visual Integration with Semantic Variance-Scaled Perturbation for Text-Video Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GAIS proposes a dual-perspective approach to text-video retrieval: a Frame-level Gated Fusion module that adaptively selects and integrates audio-visual features under text guidance for fine-grained temporal alignment, combined with a Semantic Variance-Scaled Perturbation regularizer that stabilizes text embeddings through semantics-aware perturbation. This addresses the key limitation that existing methods underutilize audio semantics and rely on coarse fusion strategies, achieving better multimodal alignment through both representation learning and regularization techniques. The framework's practical impact is improved retrieval accuracy by bridging modality gaps while enhancing embedding robustnessâ€”particularly valuable for applications requiring precise audio-visual-language correspondence in video understanding tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2510.12089" style="color:#4ea8ff;">Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Playmate2 addresses multi-character audio-driven animation using a DiT-based framework with LoRA fine-tuning and position-shift inference for efficient long-form video generation, while introducing a novel training-free Mask Classifier-Free Guidance mechanism that improves lip-sync accuracy and temporal coherence through reward feedback without additional model training. The approach combines partial parameter updates with inference-time guidance to enable scalable multi-character animation while maintaining foundation model capabilities, representing a significant step toward practical, efficient talking-head generation systems.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11614" style="color:#4ea8ff;">Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p># FPGA Resurgence in AI Acceleration

FPGAs are emerging as a viable alternative to GPUs for AI workloads requiring ultra-low latency, deterministic timing, and energy efficiency by enabling direct hardware mapping of convolutions and attention mechanisms with reconfigurable logic. Their key advantages include field-reprogrammable architecture for model-specific optimization, edge inference capability with reduced bandwidth/privacy exposure, and potential to free GPU resources by offloading latency-sensitive tasks to customized hardware pipelines. This positions FPGAs as strategic for embedded AI systems where predictable performance and hardware-algorithm co-design outweigh GPU's general-purpose flexibility.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11743" style="color:#4ea8ff;">Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 24</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>This work introduces a curiosity-driven Mixture-of-Experts framework that uses Bayesian epistemic uncertainty to route inputs across heterogeneous quantized experts (ternary BitNet, variable-bit BitLinear, post-training quantization), achieving aggressive 4-bit quantization while maintaining 99.9% accuracy on audio tasks with 4Ã— compression. The key innovation is using uncertainty estimation to reduce MoE latency variance by 82% (230msâ†’29ms std dev), solving the critical deployment challenge of unpredictable inference timing on resource-constrained devices. This approach demonstrates practical equivalence between 4-bit and full-precision models while delivering 41% energy savings versus 8-bit quantization.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11751" style="color:#4ea8ff;">Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>Concept-RuleNet proposes a neurosymbolic architecture that addresses hallucination and interpretability issues in VLMs by grounding symbolic reasoning in actual visual concepts rather than task labels aloneâ€”using a multi-agent system where a concept generator mines discriminative visual features from training data to condition symbol discovery and rule composition. This approach bridges the gap between black-box neural perception and transparent symbolic reasoning, enabling more robust out-of-distribution generalization while maintaining explainability through visual grounding in learned concepts rather than abstract labels.</p>
</article>
</body>
</html>