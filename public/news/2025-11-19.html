
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-19</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-19</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12404" style="color:#4ea8ff;">SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>SynthGuard is an open-source, multimodal detection platform that leverages both traditional detectors and multimodal LLMs to identify AI-generated images and audio while providing explainable inferenceâ€”addressing the gap left by closed-source deepfake tools lacking transparency. The platform combines unified multi-modal support with interactive analysis capabilities, enabling users to understand detection decisions rather than receiving black-box verdicts, which carries significant implications for combating synthetic media misinformation at scale. This approach represents a shift toward democratizing detection capabilities while prioritizing interpretability, critical as synthetic content generation becomes increasingly sophisticated.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14582" style="color:#4ea8ff;">OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>OmniZip introduces a training-free, audio-guided token compression framework for omnimodal LLMs that addresses the computational bottleneck of joint audio-video processing by using audio as an anchor to dynamically prune video tokens while preserving cross-modal correspondences. The method leverages audio salience and retention scores to guide spatio-temporal video compression, enabling faster inference without retraining. This approach is particularly valuable for resource-constrained deployment of multimodal models, as it handles the previously unaddressed challenge of coordinated compression across asynchronous modalities.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and" style="color:#4ea8ff;">Google unveils Gemini 3 claiming the lead in math, science, multimodal and agentic AI benchmarks </a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Google's Gemini 3 represents a comprehensive frontier model family claiming state-of-the-art performance across mathematical reasoning, scientific domains, multimodal understanding, and agentic AI tasks, with variants including a specialized "Deep Think" reasoning mode and integrated agent execution capabilities. The release emphasizes ecosystem integration across Google's developer platforms (Vertex AI, AI Studio, CLI) and introduces Antigravity, an agent-first development environment designed to simplify multi-step task orchestration. Key technical implications include improved reasoning through dedicated inference modes and native agentic architecture, positioning Gemini 3 as a competitive response to other frontier models with enhanced capabilities for complex, sequential problem-solving workflows.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#f59e0b;">âš¡ 45</span></div>
  <p>Yanyun-3 demonstrates a novel cross-platform strategy game agent combining Qwen2.5-VL's multimodal reasoning with UI-TARS for precise UI control, achieving autonomous operation across heterogeneous game environments on core tactical tasks like target localization and resource allocation. The framework's systematic evaluation of different visual input modalities (static images, sequences, videos) reveals optimal multimodal data combinations for complex human-computer interaction scenarios, establishing a foundation for generalizable VLM-based agents beyond gaming.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent tackles long-video understanding in MLLMs by introducing Schematic and Narrative Episodic Memory that structurally encodes events with their causal and temporal relations into compact context representations, circumventing token limitations and long-term dependency challenges. The framework operates through a Perception-Action-Reflection cycle with a Memory Manager for context-aware retrieval, enabling MLLMs to perform deeper video reasoning on extended sequences without the typical architectural bottlenecks. This approach has direct implications for applications requiring complex event reasoning across videos, from autonomous systems to narrative understanding tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12072" style="color:#4ea8ff;">ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#10b981;">âš¡ 92</span></div>
  <p>ProAV-DiT addresses synchronized audio-video generation by converting audio into video-like representations and projecting both modalities into a unified latent space via orthogonal decomposition, enabling fine-grained spatiotemporal alignment. The approach uses a Multi-scale Dual-stream Spatio-Temporal Autoencoder with multi-scale attention mechanisms for improved temporal coherence and modality fusion, significantly reducing computational overhead compared to standard multimodal diffusion approaches. This work is particularly relevant for applications requiring real-time or efficient sounding video synthesis where structural misalignment between audio and video modalities has been a persistent bottleneck.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>Uni-MoE-2.0-Omni introduces a dynamic-capacity Mixture-of-Experts architecture with shared/routed/null experts and Omni-Modality 3D RoPE for spatio-temporal alignment, enabling efficient omnimodal understanding and generation (images, text, speech) across 10 cross-modal input types. The model leverages progressive training with iterative reinforcement and curated multimodal data matching on top of Qwen2.5-7B, advancing language-centric reasoning while maintaining computational efficiency through intelligent expert routing. This open-source approach standardizes the MoE pattern for omnimodal systems, making scalable cross-modal architectures more accessible to researchers beyond vision-language models.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13530" style="color:#4ea8ff;">Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>Researchers propose a multimodal dataset collection protocol for detecting social anxiety manifestations in human-robot interaction contexts, leveraging synchronized audio, video, and physiological signals from 70+ participants stratified by anxiety levels during Wizard-of-Oz interactions. This addresses a critical gap in affect-recognition datasets, enabling development of anxiety-adaptive robotic systems that can dynamically adjust interaction strategies based on real-time affective state inference. The complementary nature of multimodal signals (vocal prosody, facial expressions, autonomic markers) should significantly improve robustness of anxiety detection over single-modality approaches, with direct applications in therapeutic and social robotics.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM introduces an end-to-end framework combining a novel Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal LLMs (like Qwen2.5-Omni-7B) to directly infer speaker intent from raw audio, addressing the challenge of modeling complex inter-speaker dependencies in dialogue. The key innovation is an adaptive semi-supervised learning strategy using dual-threshold confidence filtering and entropy-based sample selection to generate reliable pseudo-labels from unlabeled data, effectively tackling data scarcity. This approach enables practical deployment for intent recognition in long dialogues with minimal annotation overhead while maintaining state-of-the-art performance.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.14143" style="color:#4ea8ff;">SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>SMART introduces shot-aware temporal structure and audio-enhanced multimodal fusion to video moment retrieval, using selective token compression within shots to reduce redundancy while preserving fine-grained temporal detailsâ€”addressing the limitation of existing MLLM-based approaches that rely on coarse visual-only understanding. The key innovation is leveraging audio cues alongside vision and applying hierarchical temporal modeling at the shot level rather than frame level, enabling more efficient processing of untrimmed videos with complex temporal dynamics.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.01711" style="color:#4ea8ff;">GAIS: Frame-Level Gated Audio-Visual Integration with Semantic Variance-Scaled Perturbation for Text-Video Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 15</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GAIS introduces a dual-pronged approach to text-video retrieval by combining Frame-level Gated Fusion (FGF) for adaptive audio-visual integration under textual guidance with Semantic Variance-Scaled Perturbation (SVSP) to regularize embeddings in a semantics-aware manner. The key innovation is leveraging fine-grained temporal selection of frames while addressing the critical underutilization of audio semantics in existing coarse-fusion methods. This complementary architecture should significantly improve multimodal alignment and embedding robustness for retrieval tasks where precise audio-visual-language correspondence is essential.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2510.12089" style="color:#4ea8ff;">Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Playmate2 addresses multi-character audio-driven animation using a DiT-based framework with LoRA fine-tuning and position-shift inference for arbitrary-length video generation while maintaining lip-sync accuracy and temporal coherence. The key innovation is a training-free Mask Classifier-Free Guidance approach combined with reward feedback optimization, enabling efficient multi-character animation without requiring full model retraining. This represents a significant step toward practical, scalable solutions for long-form talking-head video synthesis with improved synchronization and natural motion generation.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11614" style="color:#4ea8ff;">Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>FPGAs are emerging as a compelling alternative to GPUs for AI workloads requiring sub-millisecond latency, deterministic performance, and edge deployment, enabling direct hardware mapping of convolutions and attention mechanisms with superior power efficiency and customization. Unlike fixed GPU architectures, FPGAs' reconfigurability allows algorithmic optimization at the circuit level, SoC integration with embedded processors, and on-device inference that reduces bandwidth and privacy risks while freeing GPU resources for other tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11743" style="color:#4ea8ff;">Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>This paper presents a Bayesian uncertainty-guided Mixture-of-Experts approach that routes inputs across heterogeneous quantized experts (BitNet ternary, variable-bit BitLinear, post-training quantization) to achieve aggressive 4-bit compression while maintaining 99.9% accuracy on audio tasks. The key innovation is using epistemic uncertainty for routing decisions, which dramatically reduces inference latency variance by 82% (230msâ†’29ms std dev), solving a critical stability problem for edge deployment where prediction latency unpredictability is often more problematic than mean latency. This combines model compression with inference reliability, delivering 4x compression + 41% energy savings for resource-constrained audio applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11751" style="color:#4ea8ff;">Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-19
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 62</span></div>
  <p>Concept-RuleNet addresses hallucination and interpretability in VLMs by coupling neural perception with neurosymbolic reasoning through a multi-agent architecture that grounds symbolic rules in actual visual concepts rather than just task labels. The key innovation is mining discriminative visual concepts directly from training images to condition symbol discovery, reducing label bias and improving out-of-distribution robustness while maintaining transparent, executable reasoning. This approach bridges the explainability-accuracy gap by ensuring the symbolic reasoning layer operates on semantically grounded representations tied to visual statistics rather than abstract labels.</p>
</article>
</body>
</html>