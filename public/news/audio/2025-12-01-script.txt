Chris: Alright, we've got three different research teams all dropping massive multimodal models in the same week. LongCat-Flash-Omni is 560 billion parameters, LFM2 is going up to 8.3 billion, and then there's this whole thing about modalities not playing nice together.

Lily: [pauses] 560 billion parameters. Let me guess - they're calling it "efficient" because only 27 billion are active at once?

Chris: Yeah, it's a Mixture of Experts architecture. The pitch is real-time audio-visual interaction despite the scale. But here's what actually matters - they're using curriculum-based progressive training and this modality-decoupled parallelism scheme to keep latency down.

Lily: Okay, but who the hell is deploying a 560 billion parameter model? Even with MoE, you're looking at serious infrastructure costs. What's the actual use case that justifies this?

Chris: That's where it gets interesting. The fourth paper basically says all these multimodal models are broken when modalities conflict. They built this benchmark called MMA-Bench that shows current systems completely shit the bed when audio and visual information don't align.

Lily: So we're building bigger models before we've solved the fundamental problem of how they handle conflicting inputs?

Chris: Exactly. And the researchers are proposing modality alignment tuning - basically teaching models explicit prioritization rules. Which sounds an awful lot like admitting we don't actually understand how these things integrate information.

Lily: Right. Let's talk about LFM2 then, because at least they're being honest about constraints. They're optimizing for CPU inference, which is refreshing.

Chris: Yeah, this one's actually practical. They're doing hardware-aware architecture search, combining gated short convolutions with grouped query attention. Getting 2x CPU speedup over comparable models.

Lily: That's the kind of optimization that matters for actual deployment. Most companies aren't running everything on A100s. What's the training approach?

Chris: Three-stage post-training pipeline - supervised fine-tuning, then preference optimization, then model merging. But the interesting bit is their tempered, decoupled Top-K knowledge distillation to prevent support mismatch.

Lily: [pauses] Support mismatch between what, the teacher and student models?

Chris: Right. When you're distilling knowledge from a larger model, if the probability distributions don't overlap properly, you get garbage. They're using temperature scaling and Top-K filtering to keep the student model from learning nonsense.

Lily: Okay, so between LFM2's edge optimization and LongCat's massive MoE approach, we're seeing two completely different philosophies. One is saying "let's make this work on actual hardware people have," and the other is saying "fuck it, bigger is better, we'll figure out deployment later."

Chris: And then there's the fMRI paper, which is honestly the wildest one here.

Lily: Yeah, I was wondering when we'd get to brain scanning. They're trying to decode thoughts into language?

Chris: Not quite that dramatic, but close. fMRI-LM is tokenizing brain activity into language-aligned representations, then fine-tuning LLMs to jointly model brain tokens and text. The innovation is they built this large descriptive corpus mapping imaging features to structured text.

Lily: [pauses] So they're treating brain scans like another modality input to a language model?

Chris: Exactly. First application of multimodal LLM reasoning to neuroimaging. They can do both temporal prediction of neural activity and linguistic interpretation of what the brain is processing.

Lily: Okay, but let's be real - what's the training data situation? You can't exactly scrape the internet for paired fMRI-text datasets.

Chris: That's the bottleneck they claim to solve. They're constructing synthetic paired data by mapping known imaging features to descriptions. Which is clever, but also means the model is learning correlations that researchers have already documented, not discovering new ones.

Lily: So it's not actually reading minds, it's pattern matching against existing neuroscience literature encoded into the training data.

Chris: Right. Though to be fair, if it works, it could speed up analysis of fMRI studies significantly. The semantic decoding of brain cognition could be useful for research, even if we're years away from practical applications.

Lily: Let me ask the business question - who's funding this? Because I don't see a clear commercial path for brain-scanning LLMs.

Chris: Academic research, probably grant-funded. This is the kind of thing that generates papers and maybe leads somewhere in five to ten years. It's not solving a business problem today.

Lily: Unlike the pedestrian classification paper, which is actually addressing a real problem.

Chris: Yeah, CNN-based age and gender classification from low-resolution surveillance footage. They're framing it as a six-class problem - different age groups and genders - using full-body features instead of facial recognition.

Lily: Which is necessary for surveillance cameras that are actually deployed in the real world. You're not getting high-res facial data from intersection cameras in developing countries.

Chris: Exactly. They're using ResNet50, which is not exactly cutting edge, but it's proven and efficient. The use case is pedestrian safety monitoring in low and middle-income countries with mixed traffic.

Lily: Okay, but let's talk about the ethics here. Demographic classification from surveillance footage? That's a minefield.

Chris: Fair point. The paper frames it as identifying vulnerable populations in high-risk environments. Kids and elderly people need different safety interventions at intersections. But yeah, the technology could obviously be misused.

Lily: And there's no technical safeguard against deployment for other purposes. Once you've built a system that classifies people by demographics from surveillance footage, you can't control how it's used.

Chris: [sighs] Right. Though I'd argue the technology already exists - they're just applying standard CNNs to a specific dataset. The cat's out of the bag on this stuff.

Lily: Let's go back to the modality conflict paper, because I think that's the most important one for anyone actually building multimodal systems.

Chris: Agreed. MMA-Bench is testing what happens when audio and visual information contradict each other, or when you have adversarial text inputs. And current models are failing badly.

Lily: What does failure look like in practice?

Chris: The model either ignores one modality completely, or produces outputs that don't make sense given either input. There's no consistent strategy for resolution. Sometimes visual dominates, sometimes audio, sometimes it just hallucinates a third option.

Lily: So if you're building a product that relies on multimodal input - say, a video analysis tool or a virtual assistant - you can't actually trust the model to handle conflicting information correctly.

Chris: Not without additional training. The paper's solution is modality alignment tuning - explicitly teaching the model rules for prioritization. But that means you need to know in advance which modality should be trusted in which scenarios.

Lily: Which requires domain expertise and potentially different fine-tuning for different applications. That's not a general solution.

Chris: No, it's a workaround. The fundamental issue is we're scaling these models without understanding how they integrate information. We're just throwing more parameters at the problem.

Lily: [pauses] Okay, so taking a step back - what should people actually pay attention to from this batch of papers?

Chris: If you're deploying models on edge devices or CPUs, LFM2's optimization techniques are directly applicable. The hardware-aware architecture search and efficient distillation methods are solving real constraints.

Lily: And if you're building multimodal systems, you need to test for modality conflicts. Don't assume the model will handle ambiguous or contradictory inputs gracefully, because the research shows it won't.

Chris: The LongCat work is interesting from a research perspective - they're pushing the boundaries of what's possible with MoE architectures. But unless you have the infrastructure to support 560 billion parameters, it's not immediately practical.

Lily: And the fMRI paper is pure research. Fascinating, potentially important long-term, but not something anyone should be planning products around.

Chris: The pedestrian classification is interesting because it's solving a real problem with existing technology. It's not sexy, but it's the kind of applied work that actually gets deployed.

Lily: Though with significant ethical considerations that aren't really addressed in the paper.

Chris: Right. Which is pretty typical for technical papers, unfortunately. They focus on the "can we do this" without spending much time on "should we do this."

Lily: Alright, so the pattern I'm seeing is we've got one paper addressing actual deployment constraints, one exposing fundamental problems with current approaches, two pushing scale without clear justification, and one that's solving a narrow applied problem.

Chris: Yeah, that's a fair summary. The field is still figuring out what works at scale. There's a lot of throwing things at the wall and seeing what sticks.

Lily: And the modality conflict issue suggests we're building on shaky foundations. If the models can't reliably handle conflicting inputs, that's going to cause real problems in production.

Chris: Absolutely. You can have the most efficient architecture in the world, but if it gives you wrong answers when modalities don't align perfectly, you can't trust it for critical applications.

Lily: Which brings us back to the same conversation we keep having - impressive benchmarks don't mean robust systems.

Chris: [pauses] Yeah. And until we solve the robustness problem, a lot of this research is going to stay in academic papers rather than production systems.

Lily: Alright, that's probably enough for today. Multi-modal chaos and brain scanning LLMs.

Chris: Just another week in AI research.