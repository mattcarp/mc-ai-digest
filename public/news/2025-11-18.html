
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-18</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-18</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://www.marktechpost.com/2025/11/17/uni-moe-2-0-omni-an-open-qwen2-5-7b-based-omnimodal-moe-for-text-image-audio-and-video-understanding/" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: An Open Qwen2.5-7B Based Omnimodal MoE for Text, Image, Audio and Video Understanding</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    MarkTechPost
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Uni-MoE-2.0-Omni is an open-source omnimodal mixture-of-experts model built on Qwen2.5-7B that unifies text, image, audio, and video understanding through language-centric reasoning, enabling efficient multimodal processing without requiring massive parameter counts. The architecture leverages MoE routing to selectively activate specialized pathways for different modalities, balancing computational efficiency with comprehensive perceptual understanding across all major input types. This approach is particularly significant for researchers seeking production-ready, fully open omnimodal systems that can handle realistic multimodal tasks without vendor lock-in.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM proposes an end-to-end framework combining a novel Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal foundation models (Qwen2.5-Omni-7B) to directly infer speaker intent from audio by modeling complex inter-dependencies in multi-speaker dialogues. The approach introduces an adaptive semi-supervised learning strategy featuring confidence-aware pseudo-label generation with dual-threshold filtering and entropy-based sample selection to address scarce annotated data challenges. This addresses a practical bottleneck in dialogue understanding by enabling direct acoustic-to-intent inference without separate speech recognition, potentially improving latency and error propagation in real-world applications like market call analysis.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11910" style="color:#4ea8ff;">Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**Query-Aware Token Selector (QTSplus)** addresses the quadratic complexity problem in long-video MLLMs by dynamically selecting task-relevant visual tokens based on input queries, using cross-attention scoring and query-complexity-aware retention budgets to reduce tokens from linear-to-video-length scaling. The method employs differentiable straight-through estimation for training and hard gating at inference, enabling efficient long-video understanding without retraining vision encoders. This approach is particularly valuable for production systems where attention cost, memory, and latency scale prohibitively with video duration.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent addresses long-video understanding in MLLMs by introducing Schematic and Narrative Episodic Memoryâ€”a structured representation that models events, causal relations, and temporal dependencies into a compact contextâ€”circumventing token limitations while enabling deeper temporal reasoning. The framework employs a Perception-Action-Reflection cycle with a Memory Manager component for selective retrieval, enabling robust context-aware inference across extended sequences that standard transformer architectures struggle to handle.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13655" style="color:#4ea8ff;">OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>OlmoEarth introduces a specialized multimodal foundation model tailored to Earth observation's unique spatio-temporal and multi-sensor nature, employing novel self-supervised learning formulations and masking strategies that outperform 12 competing models across 24 benchmarks. The system achieves state-of-the-art embedding and fine-tuning performance (15/24 and 19/29 tasks respectively) while being deployed as an accessible platform for NGOs and non-profits, democratizing advanced geospatial AI capabilities beyond traditional research institutions.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Uni-MoE-2.0-Omni advances open-source omnimodal AI by implementing a dynamic-capacity MoE architecture with shared/routed/null experts and 3D RoPE for spatio-temporal cross-modal alignment, enabling efficient processing across 10 modality types while maintaining language-centric reasoning. The approach combines progressive training with iterative reinforcement and curated multimodal data matching, achieving omnimodal understanding and generation (images, text, speech) from a Qwen2.5-7B foundationâ€”balancing computational efficiency with multi-task capability.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#f59e0b;">âš¡ 45</span></div>
  <p>Yanyun-3 demonstrates the first cross-platform strategy game agent by combining Qwen2.5-VL's multimodal reasoning with UI-TARS for precise UI interaction, successfully automating complex tasks like target localization and resource allocation across heterogeneous game environments. The framework's systematic evaluation of different visual input modalities (static images, sequences, videos) provides valuable insights into multimodal data requirements for robust generalization across diverse interfaces and dynamic gameplay conditions. This work represents a significant step toward VLM-based agents capable of handling real-world human-computer interaction complexity beyond controlled benchmarks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.04953" style="color:#4ea8ff;">APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>APVR introduces a training-free, two-stage hierarchical retrieval framework that enables hour-level video understanding in MLLMs by selectively preserving critical visual information through Pivot Frame Retrieval (using query expansion and spatio-semantic scoring) and Pivot Token Retrieval (query-aware token selection), effectively bypassing memory constraints without requiring model retraining. This approach balances the resource efficiency of prior compression methods with superior performance by maintaining semantic completeness through adaptive, query-guided information retention rather than static downsampling. The hierarchical design targets both the computational and memory bottlenecks that currently limit long-form video processing in production MLLMs, offering practical scalability for industrial applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>READ addresses the critical inference speed bottleneck in diffusion-based talking head generation through a three-component pipeline: temporal VAE compression for video latents, a Speech Autoencoder for aligned audio latent codes, and an efficient Audio-to-Video Diffusion Transformer backbone. This architecture achieves real-time synthesis by operating in highly compressed latent spaces, dramatically reducing computational overhead while maintaining audio-visual synchronizationâ€”a key requirement for practical deployment of generative talking head systems.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    eess.AS updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#10b981;">âš¡ 87</span></div>
  <p>READ addresses the critical bottleneck of diffusion models in talking head generation by introducing temporal VAE compression for video latents and a Speech Autoencoder that aligns audio-visual representations in compressed space, enabling real-time inference through an optimized Audio-to-Video Diffusion Transformer. This approach achieves practical deployment feasibility while maintaining diffusion model quality by reducing token count substantially and synchronizing speech/video at the latent level rather than raw data. The framework represents a significant efficiency-quality tradeoff advancement, making diffusion-based talking head generation viable for real-time applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.10979" style="color:#4ea8ff;">PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Video LLMs experience temporal instability due to RoPE's multimodal extension creating rippling artifacts in the Fourier time kernel, causing frame-timing shifts to dramatically alter attention patterns. PAS (Phase Aggregated Smoothing) addresses this through a training-free, inference-time technique applying phase offsets across attention heads before aggregation, effectively smoothing the temporal kernel while preserving per-head spectrum magnitude and avoiding architectural changes. This work identifies a fundamental positional encoding vulnerability in video transformers and provides a practical, parameter-free solution applicable to existing deployed models.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11006" style="color:#4ea8ff;">MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>MSMT-FN introduces a multi-segment multi-task fusion architecture specifically optimized for extracting purchasing intent from marketing call audio through joint sentiment and emotion analysis. The approach validates against multiple benchmarks (CMU-MOSI/MOSEI, MELD) while introducing MarketCalls, a proprietary dataset targeting the practical challenge of classifying customer attitudes at scale in commercial contexts. This work bridges multimodal sentiment analysis with real-world business applications, offering researchers an open-sourced baseline for audio-based intent detection in high-volume conversation datasets.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11124" style="color:#4ea8ff;">AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> <span class="score-badge" style="background:#10b981;">âš¡ 85</span></div>
  <p>AV-Dialog introduces the first multimodal dialogue system leveraging audio-visual fusion for robust speaker tracking, turn-taking prediction, and response generation in noisy multi-speaker environments. The framework combines acoustic tokenization with multi-task, multi-stage training across monadic, synthetic, and real audio-visual datasets, demonstrating significant improvements in streaming transcription accuracy and turn-boundary detection compared to audio-only baselines. This work establishes visual speaker identification and lip-sync awareness as critical components for building naturally-flowing conversational agents in challenging acoustic conditions.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2412.00060" style="color:#4ea8ff;">MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>MOSABench introduces a specialized evaluation dataset of ~1,000 multi-object images to assess MLLMs' capability for fine-grained, per-object sentiment analysisâ€”a capability gap in current benchmarks despite MLLMs' advances in high-level vision-language tasks. The benchmark incorporates technical innovations including distance-based target annotation and standardized post-processing mechanisms to handle the complexity of independently scoring sentiment for multiple objects in cluttered scenes. Experiments expose significant limitations in existing MLLMs (mPLUG-owl, Qwen variants), suggesting that robust multi-object semantic understanding at the instance level remains an open challenge requiring architectural or training improvements.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.03662" style="color:#4ea8ff;">Zero-Shot Temporal Interaction Localization for Egocentric Videos</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**EgoLoc** addresses zero-shot temporal interaction localization in egocentric videos by introducing a self-adaptive sampling strategy that generates optimized visual prompts for VLMs, moving beyond the coarse-grained, open-loop limitations of existing zero-shot temporal action localization methods. This approach eliminates reliance on annotated interaction categories, enabling better domain generalization and deployment efficiency for tasks like human-robot skill transfer and behavior analysis. The key innovation centers on fine-grained grasp action localization through adaptive visual prompting rather than traditional supervised fine-tuning.</p>
</article>
</body>
</html>