
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-18</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-18</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://www.marktechpost.com/2025/11/17/uni-moe-2-0-omni-an-open-qwen2-5-7b-based-omnimodal-moe-for-text-image-audio-and-video-understanding/" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: An Open Qwen2.5-7B Based Omnimodal MoE for Text, Image, Audio and Video Understanding</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    MarkTechPost
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Uni-MoE-2.0-Omni is an open-source omnimodal mixture-of-experts model built on Qwen2.5-7B that unifies text, image, audio, and video understanding through a language-centric reasoning architecture, enabling efficient multimodal processing without massive parameter overhead. The MoE approach allows selective expert activation across modalities, reducing computational requirements while maintaining strong cross-modal reasoning capabilitiesâ€”a pragmatic solution for deploying truly omnimodal systems at scale.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM addresses audio dialogue intent recognition by combining a Multi-Relational Dialogue Attention Network (MR-DAN) with multimodal foundation models (Qwen2.5-Omni-7B) for end-to-end acoustic-to-intent processing, eliminating intermediate transcription steps. The framework incorporates an adaptive semi-supervised learning strategy featuring confidence-aware pseudo-labeling with dual-threshold filtering and entropy-based sample selection to mitigate data scarcityâ€”a critical bottleneck in dialogue understanding tasks. Evaluated on proprietary and public benchmarks, this approach demonstrates how graph-informed attention mechanisms combined with LLM reasoning can improve speaker intent extraction in complex multi-turn conversations.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11910" style="color:#4ea8ff;">Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>This paper introduces QTSplus, a query-aware visual token selection module that addresses the quadratic attention complexity bottleneck in long-video MLLMs by dynamically filtering vision tokens based on query relevance using cross-attention scoring and instance-specific retention budgeting. The approach employs a differentiable straight-through estimator during training and hard gating at inference to enable end-to-end optimization while maintaining computational efficiency, effectively converting the linear token growth problem into adaptive, query-dependent token pruning. This technique enables practical long-video understanding by reducing memory, latency, and attention costs without requiring architectural changes to underlying LLMs.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent introduces a novel episodic memory architecture that explicitly models causal and temporal event relationships to overcome MLLMs' token limitations in long-video understanding, using a Perception-Action-Reflection cycle with a Memory Manager for contextual retrieval. The key innovation is replacing flat token sequences with structured schematic and narrative memory that captures global context and complex event dependencies, fundamentally addressing long-term temporal reasoning challenges in video comprehension. This approach has significant implications for video QA, summarization, and reasoning tasks where maintaining coherent event relationships across extended sequences is critical.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13655" style="color:#4ea8ff;">OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>OlmoEarth introduces a specialized multimodal foundation model tailored for Earth observation data, combining spatial, sequential, and temporal dimensions through custom self-supervised learning, masking strategies, and domain-specific loss functionsâ€”outperforming 12 competing models across 24 benchmarks with state-of-the-art embedding and fine-tuning results. The work includes an end-to-end platform democratizing access to frontier Earth observation capabilities for NGOs and nonprofits, addressing the unique challenges of satellite/geospatial data that traditional vision models struggle with due to its hybrid image-video-multimodal nature.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#10b981;">âš¡ 85</span></div>
  <p>Uni-MoE-2.0-Omni introduces a dynamic-capacity Mixture-of-Experts architecture with shared/routed/null expert routing and Omni-Modality 3D RoPE for spatio-temporal alignment, enabling efficient omnimodal (text, image, speech) understanding and generation from a Qwen2.5-7B base. The model leverages progressive training with iterative reinforcement and curated cross-modal data matching to balance computational efficiency across 10 input modalities, advancing language-centric multimodal reasoning as a fully open-source alternative to proprietary omnimodal systems.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#f59e0b;">âš¡ 45</span></div>
  <p>Yanyun-3 introduces a novel VLM-based agent framework that achieves cross-platform strategy game automation by combining Qwen2.5-VL's multimodal reasoning with UI-TARS for precise UI interaction, successfully handling complex tasks like target localization and resource allocation across heterogeneous game environments. The work demonstrates that VLMs can effectively generalize across diverse interfaces through systematic evaluation of multimodal inputs (static images, sequences, and video), establishing a significant benchmark for autonomous operation in dynamic, partially observable domains that closely mirror real-world human-computer interaction challenges.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.04953" style="color:#4ea8ff;">APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>APVR introduces a training-free, hierarchical retrieval framework for hour-level video understanding in MLLMs, addressing memory constraints through dual-stage filtering: Pivot Frame Retrieval uses query-expanded semantic scoring to identify relevant frames, while Pivot Token Retrieval performs query-aware token selection to preserve critical visual information. This approach overcomes traditional bottlenecks by maintaining sufficient contextual information without retraining, enabling efficient processing of long-form video content where previous compression-based methods sacrificed performance.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>READ addresses the critical inference speed bottleneck in diffusion-based talking head generation through a three-pronged approach: temporal VAE-based video compression to reduce token overhead, a novel Speech Autoencoder for temporally-aligned audio-visual representations, and a custom Audio-to-Video Diffusion Transformer backbone optimized for real-time synthesis. This combination enables practical deployment of diffusion models for audio-driven video generation while maintaining quality, a significant advancement over prohibitively slow existing diffusion approaches.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    eess.AS updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#10b981;">âš¡ 85</span></div>
  <p>READ addresses the critical inference speed bottleneck in diffusion-based talking head generation through three key innovations: (1) temporal VAE compression to reduce token count, (2) a Speech Autoencoder that aligns audio to compressed video latent space, and (3) a specialized Audio-to-Video Diffusion Transformer backbone for efficient synthesis. This approach enables real-time generation while maintaining audio-visual synchronization, significantly advancing the practical deployment of diffusion models for video synthesis applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.10979" style="color:#4ea8ff;">PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>**Summary:**

Video LLMs exhibit temporal instability where minor frame timing shifts cause attention collapse due to ripples in the Fourier-domain time kernel induced by multimodal RoPEâ€”a fundamental architectural issue. The authors propose Phase Aggregated Smoothing (PAS), a training-free post-hoc mechanism that applies phase offsets across attention heads and aggregates outputs to smooth the temporal kernel, effectively decoupling frame attention from positional encoding artifacts without model retraining.

**Key Implications:** This is a practical, zero-cost stabilization technique addressing a critical but overlooked architectural vulnerability in video LLMs; the phase-space analysis revealing RoPE's temporal instability as a Fourier kernel problem opens new directions for debugging multimodal positional encoding designs.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11006" style="color:#4ea8ff;">MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>MSMT-FN introduces a multi-segment multi-task fusion architecture specifically optimized for extracting purchasing intent from marketing call audio, addressing the practical challenge of classifying customer sentiment at scale. The approach validates strongly across both a newly released proprietary MarketCalls dataset and established multimodal benchmarks (CMU-MOSI/MOSEI, MELD), suggesting the fusion strategy generalizes beyond domain-specific applications. The open-sourcing of both code and dataset could accelerate adoption of task-specific audio classification methods in commercial sentiment analysis pipelines.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11124" style="color:#4ea8ff;">AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>AV-Dialog introduces the first audio-visual dialogue framework that leverages visual speaker tracking and lip-sync cues alongside acoustic information to improve robustness in noisy, multi-speaker environmentsâ€”addressing fundamental limitations of audio-only dialogue systems through multi-task, multi-stage training on diverse datasets. The approach combines acoustic tokenization with speaker-aware turn-taking prediction and generates contextually coherent responses, demonstrating significant improvements in transcription accuracy and dialogue quality compared to baseline audio-only models. This work establishes that visual grounding is critical for speaker identification and turn-taking in conversational AI, enabling more natural dialogue agents suited for real-world deployment scenarios.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2412.00060" style="color:#4ea8ff;">MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>MOSABench introduces a standardized benchmark with ~1,000 multi-object images to evaluate MLLMs' capability in fine-grained sentiment analysis at the object level rather than image-level, addressing a significant gap in current evaluation methodologies. The benchmark incorporates distance-based target annotation and improved scoring mechanisms to handle the complexity of independently assessing sentiment for multiple entities within single images. Experiments expose critical limitations in state-of-the-art MLLMs like mPLUG-owl, suggesting that while these models excel at holistic semantic tasks, they struggle with granular multi-object reasoningâ€”an important insight for advancing MLLM architectures toward real-world applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.03662" style="color:#4ea8ff;">Zero-Shot Temporal Interaction Localization for Egocentric Videos</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>**Zero-Shot Temporal Interaction Localization for Egocentric Videos**

EgoLoc introduces a zero-shot approach for localizing human-object interaction (HOI) timings in egocentric video by leveraging vision-language models with a self-adaptive sampling strategy to generate contextually relevant visual prompts, eliminating the need for annotated action/object categories. This addresses key limitations of prior zero-shot temporal action localization methodsâ€”their coarse-grained predictions and open-loop architecturesâ€”enabling more precise, fine-grained interaction localization applicable to behavior analysis and robot skill transfer without domain-specific retraining.</p>
</article>
</body>
</html>