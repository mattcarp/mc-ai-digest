
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Daily Digest â€“ 2025-11-18</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { background:#0b0c10; color:#e5e5e5; font-family:system-ui; padding:2rem; }
    a { color:#4ea8ff; }
    .score-badge {
      display:inline-block;
      color:#fff;
      padding:2px 10px;
      border-radius:4px;
      font-size:0.75rem;
      margin-right:0.5rem;
      font-weight:500;
    }
  </style>
</head>
<body>
  <h1>AI Daily Digest â€“ 2025-11-18</h1>
  
<article style="margin-bottom:1.5rem;">
  <h2><a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and" style="color:#4ea8ff;">Google unveils Gemini 3 claiming the lead in math, science, multimodal and agentic AI benchmarks </a></h2>
  <p style="color:#888;font-size:0.85rem;">
    AI | VentureBeat
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Google's Gemini 3 family represents a comprehensive frontier model release with claimed performance leadership across mathematical reasoning, scientific tasks, multimodal understanding, and agentic capabilities, delivered through closed-source APIs integrated into Google's ecosystem (Vertex AI, Studio, CLI). The portfolio includes specialized variantsâ€”Gemini 3 Pro as flagship, Deep Think for enhanced reasoning, and a native Gemini Agent for multi-step orchestrationâ€”alongside tooling like Antigravity, Google's agent-first development environment. Key technical implication: the emphasis on reasoning modes and agent-native architecture signals industry movement toward compositional AI systems capable of autonomous task decomposition rather than single-turn inference.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11000" style="color:#4ea8ff;">DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 25</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>DialogGraph-LLM introduces a multi-modal end-to-end framework combining a novel Multi-Relational Dialogue Attention Network (MR-DAN) with foundation models like Qwen2.5-Omni to directly map audio dialogues to speaker intents, addressing the challenge of modeling complex inter-speaker dependencies. The key innovation is an adaptive semi-supervised learning strategy employing dual-threshold confidence filtering and entropy-based sample selection for pseudo-label generation, enabling effective learning from scarce annotated dialogue data. This approach demonstrates practical value for real-world applications like call center analysis while establishing a benchmark on both proprietary and public datasets for dialogue understanding tasks.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11910" style="color:#4ea8ff;">Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#10b981;">âš¡ 82</span></div>
  <p>QTSplus addresses the quadratic attention bottleneck in long-video MLLMs by introducing query-aware dynamic token selection that reduces vision tokens based on query complexity, using cross-attention scoring and differentiable gating to maintain performance while dramatically cutting computational overhead. This approach elegantly decouples token retention from video length, enabling scalable multimodal processing for extended sequences without sacrificing the model's ability to capture relevant visual context. The instance-specific retention budget mechanism is particularly novelâ€”it recognizes that different queries require different levels of visual detail, offering a principled alternative to fixed compression ratios.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12027" style="color:#4ea8ff;">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>GCAgent introduces a novel agent framework addressing long-video understanding in MLLMs by implementing Schematic and Narrative Episodic Memoryâ€”a structured memory system that models events and their causal/temporal relationships to overcome token limitations and long-term dependency challenges. The system operates through a multi-stage Perception-Action-Reflection cycle with a Memory Manager component that retrieves contextually relevant episodic information for improved reasoning. This approach fundamentally restructures how temporal context is represented and accessed, enabling more coherent video reasoning beyond existing capacity constraints.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.13655" style="color:#4ea8ff;">OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 35</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>OlmoEarth introduces a specialized spatio-temporal foundation model that unifies Earth observation's unique data characteristicsâ€”spatial imagery, temporal sequences, and multimodal inputsâ€”through custom self-supervised learning, masking strategies, and domain-specific loss functions, achieving SOTA results on 15/24 embedding tasks and 19/29 fine-tuning benchmarks. The work is particularly significant for practitioners because it's deployed as an accessible end-to-end platform democratizing frontier Earth observation AI for NGOs and non-profits, rather than remaining confined to well-resourced institutions.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12609" style="color:#4ea8ff;">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>Uni-MoE-2.0-Omni presents a language-centric omnimodal model built on Qwen2.5-7B that introduces dynamic-capacity MoE with shared/routed/null experts for efficient cross-modal routing across 10 input modalities, coupled with Omni-Modality 3D RoPE for spatio-temporal attention alignment. The model combines progressive multi-stage training with iterative reinforcement learning and carefully curated multimodal datasets to achieve unified understanding and generation of images, text, and speechâ€”advancing the open-source landscape for production-grade omnimodal systems with computational efficiency trade-offs.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.12937" style="color:#4ea8ff;">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 23</span> <span class="score-badge" style="background:#6b7280;">âš¡ 35</span></div>
  <p>Yanyun-3 presents the first cross-platform strategy game agent combining Qwen2.5-VL's multimodal reasoning with UI-TARS for precise UI interaction, successfully generalizing across heterogeneous game environments for complex tasks like combat resource allocation and target localization. The framework's systematic evaluation of multimodal input types (static images, sequences, videos) reveals insights into optimal data representations for VLM-based game automation, addressing the long-standing challenge of robust generalization across diverse UI paradigms in dynamic environments.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.04953" style="color:#4ea8ff;">APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 22</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>APVR introduces a training-free, hierarchical retrieval framework for hour-level video understanding in MLLMs, using dual-stage pivot retrieval (frames + tokens) with query expansion and spatio-semantic confidence scoring to overcome memory constraints while maintaining visual information fidelity. The approach bypasses the performance limitations of prior compression-based methods by selectively preserving task-relevant visual features rather than uniformly downsampling, enabling practical long-video reasoning without additional model training. This addresses a critical bottleneck for production deployment of video MLLMs on resource-constrained systems.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.CV updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>READ addresses the critical bottleneck of diffusion model inference speed in talking head generation by introducing a two-stage compression strategy: temporal VAE for video latents and a novel Speech Autoencoder for temporally-aligned audio latents, combined with an optimized Audio-to-Video Diffusion Transformer backbone. This approach achieves real-time synthesis by reducing token complexity while maintaining audio-visual synchronizationâ€”a key practical requirement for deployment. The work demonstrates how architectural innovations (compressed latent spaces + specialized diffusion transformer) can make diffusion-based video synthesis practical for interactive applications.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2508.03457" style="color:#4ea8ff;">READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    eess.AS updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> </div>
  <p>READ addresses the critical inference speed bottleneck in diffusion-based talking head generation through a three-part technical innovation: temporal VAE compression of video latents, a Speech Autoencoder for aligned audio representation, and an optimized Audio-to-Video Diffusion Transformer backbone. This approach dramatically reduces computational overhead while maintaining audio-visual synchronization, enabling real-time synthesisâ€”a significant leap toward practical deployment of diffusion models in interactive applications. The compressed latent space strategy represents a key pattern for accelerating other latent diffusion tasks beyond talking heads.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.10979" style="color:#4ea8ff;">PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 18</span> </div>
  <p># Summary: PAS - Training-Free Temporal Stabilization for Video LLMs

Video LLMs exhibit temporal instability due to RoPE's multimodal extension creating frame-scale ripples in the Fourier time kernel, causing attention fluctuations from minor timing shifts. Phase Aggregated Smoothing (PAS) addresses this by applying opposing phase offsets across attention heads and aggregating outputs to smooth the temporal kernel, effectively decoupling position sensitivity while preserving magnitudeâ€”all without retraining. This training-free approach has immediate practical value for deploying more robust video understanding models by fixing a fundamental architectural vulnerability in temporal attention.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11006" style="color:#4ea8ff;">MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 28</span> <span class="score-badge" style="background:#3b82f6;">âš¡ 78</span></div>
  <p>MSMT-FN introduces a multi-segment multi-task fusion architecture specifically optimized for extracting customer sentiment and purchasing intent from marketing call audio, addressing the scalability challenge of processing large-scale commercial speech data. The approach demonstrates consistent improvements over SOTA methods across multiple benchmarks (CMU-MOSI, CMU-MOSEI, MELD) plus a new proprietary MarketCalls dataset, suggesting the architectural innovationsâ€”likely combining segmentation strategies with auxiliary task learningâ€”effectively capture both emotional and transactional signals from speech. The open-sourced codebase and dataset release position this as a practical foundation for enterprise audio analytics and domain-specific multimodal emotion recognition systems.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2511.11124" style="color:#4ea8ff;">AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> <span class="score-badge" style="background:#10b981;">âš¡ 85</span></div>
  <p>arXiv:2511.11124v1 Announce Type: cross 
Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker,...</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2412.00060" style="color:#4ea8ff;">MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"> <span class="score-badge" style="background:#3b82f6;">âš¡ 72</span></div>
  <p>MOSABench addresses a critical gap in MLLM evaluation by introducing a specialized benchmark for multi-object sentiment analysis with ~1,000 images, incorporating distance-based annotation and standardized post-processing to measure fine-grained sentiment understanding across multiple entities in complex scenes. The benchmark reveals significant limitations in current MLLMs (e.g., mPLUG-owl, Qwen) when handling spatially distributed sentiment targets, highlighting the need for improved compositional reasoning and granular semantic analysis in multimodal systems. This work establishes a foundation for more rigorous evaluation of MLLMs beyond high-level tasks, directly impacting real-world applications requiring nuanced multi-entity understanding.</p>
</article>

<article style="margin-bottom:1.5rem;">
  <h2><a href="https://arxiv.org/abs/2506.03662" style="color:#4ea8ff;">Zero-Shot Temporal Interaction Localization for Egocentric Videos</a></h2>
  <p style="color:#888;font-size:0.85rem;">
    cs.AI updates on arXiv.org
     Â· 2025-11-18
  </p>
  <div style="margin:0.5rem 0;"><span class="score-badge" style="background:#6b7280;">ðŸ’¼ 32</span> </div>
  <p>This paper presents **EgoLoc**, a zero-shot temporal interaction localization framework that leverages vision-language models (VLMs) with self-adaptive visual prompting to localize human-object interactions in egocentric video without requiring annotated action/object categories. The approach moves beyond coarse-grained zero-shot temporal action localization by introducing a closed-loop reasoning mechanism, addressing domain bias and improving deployment efficiency for downstream tasks like behavior analysis and robot skill transfer. The self-adaptive sampling strategy represents a practical solution to generating task-relevant visual prompts that enhance VLM reasoning accuracy for fine-grained temporal grounding of interactions.</p>
</article>
</body>
</html>